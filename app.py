import streamlit as st
import zipfile
import tempfile
import os
import shutil
import io
import re
import random
import string
from openai import OpenAI
from langdetect import detect, LangDetectException
from collections import Counter

st.set_page_config(
    page_title="Rewriter + DUPLICATOR",
    page_icon="üåêüîÑ",
    layout="wide"
)

# Session state
if 'processed' not in st.session_state:
    st.session_state.processed = False
if 'result' not in st.session_state:
    st.session_state.result = None

def generate_unique_site_names(theme, num=5):
    # –¢–µ–º–∞—Ç–∏—á–Ω—ñ –ø—Ä–µ—Ñ—ñ–∫—Å–∏
    themes = {
        '–∑–¥–æ—Ä–æ–≤'—è': ['Vital', 'Health', 'Well', 'Pure', 'Balance', 'Life', 'Energy', 'Gesund'],
        '—Å–ø–æ—Ä—Ç': ['Sport', 'Fit', 'Active', 'Power', 'Gym', 'Run', 'Athlet', 'Train'],
        '–∫—Ä–∞—Å–∞': ['Beauty', 'Glow', 'Shine', 'Elegant', 'Charm', 'Style', 'Lux', 'Fashion'],
        '—ó–∂–∞': ['Food', 'Taste', 'Delicious', 'Gourmet', 'Kitchen', 'Recipe', 'Eat', 'Flavor'],
        # –î–æ–¥–∞–π –±—ñ–ª—å—à–µ —Ç–µ–º –∑–∞ –ø–æ—Ç—Ä–µ–±–æ—é
    }
    base_words = themes.get(theme.lower(), ['Site', 'Web', 'Net', 'Pro', 'Hub'])  # –¥–µ—Ñ–æ–ª—Ç
    names = []
    for _ in range(num):
        word = random.choice(base_words)
        suffix = ''.join(random.choice(string.ascii_lowercase) for _ in range(random.randint(3, 6)))
        name = word + suffix.capitalize()
        names.append(name)
    return names

def detect_language(text: str) -> str:
    text = text.replace('\n', ' ').strip()
    if len(text) < 50:
        return "de"

    # –ï–≤—Ä–∏—Å—Ç–∏–∫–∞: –Ω—ñ–º–µ—Ü—å–∫—ñ —Å–∏–º–≤–æ–ª–∏ = de
    if re.search(r'[√§√∂√º√Ñ√ñ√ú√ü]', text):
        return "de"

    try:
        lang = detect(text)
        if lang in ['de', 'nl', 'da']:
            return "de"
        return lang
    except LangDetectException:
        return "de"

def get_site_language(html_files: list) -> str:
    langs = []
    for path in html_files:
        try:
            with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            text = re.sub(r'<[^>]+>', ' ', content)[:5000]
            lang = detect_language(text)
            langs.append(lang)
        except:
            pass
    
    if not langs:
        return "de"

    most_common = Counter(langs).most_common(1)[0][0]
    lang_map = {
        'de': '–ù—ñ–º–µ—Ü—å–∫–∞',
        'uk': '–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞',
        'ru': '–†–æ—Å—ñ–π—Å—å–∫–∞',
        'en': '–ê–Ω–≥–ª—ñ–π—Å—å–∫–∞',
        'fr': '–§—Ä–∞–Ω—Ü—É–∑—å–∫–∞'
    }
    return lang_map.get(most_common, "–ù—ñ–º–µ—Ü—å–∫–∞")

def rewrite_content(client, original_html: str, language: str, new_site_name: str) -> str:
    prompt = f"""
–¢–Ü–õ–¨–ö–ò —Ä–µ—Ñ—Ä–∞–∑—É–π –≤–∏–¥–∏–º–∏–π —Ç–µ–∫—Å—Ç –Ω–∞ –º–æ–≤—ñ '{language}' ‚Äî –∑—Ä–æ–±–∏ –π–æ–≥–æ —É–Ω—ñ–∫–∞–ª—å–Ω–∏–º, –ø—Ä–∏—Ä–æ–¥–Ω–∏–º, –ø—Ä–∏–≤–∞–±–ª–∏–≤–∏–º.
–ó–ê–ë–û–†–û–ù–ï–ù–û –±—É–¥—å-—è–∫—ñ –∑–º—ñ–Ω–∏ –∫—Ä—ñ–º —Ç–µ–∫—Å—Ç—É:
- –ù–ï –∑–º—ñ–Ω—é–≤–∞—Ç–∏ —Ç–µ–≥–∏, –∞—Ç—Ä–∏–±—É—Ç–∏, –∫–ª–∞—Å–∏, id, name, value, placeholder, action, method, onclick, src, href
- –ù–ï –ª–∞–º–∞—Ç–∏ —Ñ–æ—Ä–º–∏, input, button, select, textarea, —Å–∫—Ä–∏–ø—Ç–∏, —Å—Ç–∏–ª—ñ, –ø–æ—Å–∏–ª–∞–Ω–Ω—è, –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
- –ù–ï –¥–æ–¥–∞–≤–∞—Ç–∏/–≤–∏–¥–∞–ª—è—Ç–∏ –µ–ª–µ–º–µ–Ω—Ç–∏
- –ù–ï –∑–º—ñ–Ω—é–≤–∞—Ç–∏ JS-–∫–æ–¥, –ø–æ–¥—ñ—ó, —Å—Ç—Ä—É–∫—Ç—É—Ä—É
–ó–∞–º—ñ–Ω—é–π –¢–Ü–õ–¨–ö–ò —á–∏—Å—Ç–∏–π —Ç–µ–∫—Å—Ç –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ —Ç–µ–≥—ñ–≤ (h1-h6, p, li, span, div –∑ —Ç–µ–∫—Å—Ç–æ–º, label, option —Ç–æ—â–æ).
–ó–∞–º—ñ–Ω–∏ –Ω–∞–∑–≤—É —Å–∞–π—Ç—É –Ω–∞ '{new_site_name}' –≤—Å—é–¥–∏, –¥–µ –≤–æ–Ω–∞ –∑–≥–∞–¥—É—î—Ç—å—Å—è –≤ —Ç–µ–∫—Å—Ç—ñ.
–ö–æ–Ω—Ç–∞–∫—Ç–∏ (–∞–¥—Ä–µ—Å–∞, —Ç–µ–ª–µ—Ñ–æ–Ω) ‚Äî –∑–∞–º—ñ–Ω–∏ –Ω–∞ –≤–∏–ø–∞–¥–∫–æ–≤—ñ –ø—Ä–∞–≤–¥–æ–ø–æ–¥—ñ–±–Ω—ñ (–∞–¥—Ä–µ—Å–∞ –≤ –£–∫—Ä–∞—ó–Ω—ñ, +380 –Ω–æ–º–µ—Ä).
–Ø–∫—â–æ –∫–æ–Ω—Ç–∞–∫—Ç—ñ–≤ –Ω–µ –±—É–ª–æ ‚Äî –Ω–µ –¥–æ–¥–∞–≤–∞–π.
–ü–æ–≤–µ—Ä—Ç–∞–π –¢–Ü–õ–¨–ö–ò –ø–æ–≤–Ω–∏–π HTML –∑ –∑–∞–º—ñ–Ω–µ–Ω–∏–º —Ç–µ–∫—Å—Ç–æ–º, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω—å, –±–µ–∑ ```html —á–∏ markdown.
–û—Ä–∏–≥—ñ–Ω–∞–ª:
{original_html}
"""

    try:
        resp = client.chat.completions.create(
            model="grok-code-fast-1",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=8192,
            timeout=600
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        st.warning(f"–ü–æ–º–∏–ª–∫–∞ —Ä–µ—Ä–∞–π—Ç—É: {str(e)}. –ó–∞–ª–∏—à–∞—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª.")
        return original_html

st.title("üåê Rewriter + DUPLICATOR ‚Äî –†–µ—Ä–∞–π—Ç —Ç–µ–∫—Å—Ç—É + –ö–ª–æ–Ω—É–≤–∞–Ω–Ω—è")

with st.expander("‚ÑπÔ∏è –Ø–∫ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏", expanded=True):
    st.markdown("""
    1. –í–≤–µ–¥–∏ xAI API Key  
    2. –í–≤–µ–¥–∏ —Ç–µ–º—É —Å–∞–π—Ç—É (–¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –Ω–∞–∑–≤, –Ω–∞–ø—Ä. '–∑–¥–æ—Ä–æ–≤'—è')
    3. –ó–∞–≤–∞–Ω—Ç–∞–∂ ZIP/RAR –∞—Ä—Ö—ñ–≤(–∏) —Å–∞–π—Ç—É  
    4. –û–±–µ—Ä–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–ø—ñ–π —ñ –¥–æ–º–µ–Ω–Ω—É –∑–æ–Ω—É  
    5. –ù–∞—Ç–∏—Å–Ω–∏ –∫–Ω–æ–ø–∫—É ‚Äî –æ—Ç—Ä–∏–º–∞–π –≥–æ–ª–æ–≤–Ω–∏–π –∞—Ä—Ö—ñ–≤ –∑ 5 —É–Ω—ñ–∫–∞–ª—å–Ω–∏–º–∏ –≤–∞—Ä—ñ–∞–Ω—Ç–∞–º–∏ (–∫–æ–∂–µ–Ω –∑ —Ä–µ—Ä–∞–π—Ç–æ–º, –Ω–æ–≤–æ—é –Ω–∞–∑–≤–æ—é —ñ –¥–æ–º–µ–Ω–æ–º)
    """)

api_key = st.text_input("xAI API Key", type="password")
theme = st.text_input("–¢–µ–º–∞ —Å–∞–π—Ç—É (–¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –Ω–∞–∑–≤)", value="–∑–¥–æ—Ä–æ–≤'—è")

col1, col2 = st.columns([2, 1])

with col1:
    uploaded_files = st.file_uploader(
        "–ê—Ä—Ö—ñ–≤–∏ —Å–∞–π—Ç—ñ–≤ (ZIP/RAR)",
        type=['zip', 'rar'],
        accept_multiple_files=True
    )

with col2:
    domain_zone = st.radio("–î–æ–º–µ–Ω–Ω–∞ –∑–æ–Ω–∞:", ['.com', '.info'], horizontal=True)
    copies_count = st.number_input("–ö–æ–ø—ñ–π –Ω–∞ –∞—Ä—Ö—ñ–≤:", min_value=1, max_value=5, value=5)

if uploaded_files and api_key and theme:
    if st.button("üöÄ –°—Ç–≤–æ—Ä–∏—Ç–∏ 5 —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –∫–æ–ø—ñ–π –∑ —Ä–µ—Ä–∞–π—Ç–æ–º", type="primary"):
        client = OpenAI(api_key=api_key, base_url="https://api.x.ai/v1", timeout=600)

        temp_input = tempfile.mkdtemp()
        temp_rewritten = tempfile.mkdtemp()
        temp_clones = tempfile.mkdtemp()

        progress = st.progress(0)
        status = st.empty()

        # 1. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∞—Ä—Ö—ñ–≤—ñ–≤
        status.text("–ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∞—Ä—Ö—ñ–≤–∏...")
        archive_paths = []
        for i, f in enumerate(uploaded_files):
            path = os.path.join(temp_input, f.name)
            with open(path, 'wb') as out:
                out.write(f.getbuffer())
            archive_paths.append(path)
            progress.progress((i+1)/len(uploaded_files) * 0.1)

        # –ì–µ–Ω–µ—Ä—É—î–º–æ —É–Ω—ñ–∫–∞–ª—å–Ω—ñ –Ω–∞–∑–≤–∏ –¥–ª—è –≤—Å—ñ—Ö –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤
        unique_names = generate_unique_site_names(theme, copies_count * len(archive_paths))
        name_index = 0

        # 2. –î–ª—è –∫–æ–∂–Ω–æ–≥–æ –≤–∞—Ä—ñ–∞–Ω—Ç–∞ ‚Äî –æ–∫—Ä–µ–º–∏–π —Ä–µ—Ä–∞–π—Ç + –∑–∞–º—ñ–Ω–∞ –Ω–∞–∑–≤–∏
        status.text("–†–µ—Ä–∞–π—Ç —ñ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤...")
        master_zip_path = os.path.join(temp_clones, "duplicates.zip")
        with zipfile.ZipFile(master_zip_path, 'w', zipfile.ZIP_DEFLATED) as master_zip:
            for var_num in range(1, copies_count + 1):
                status.text(f"–í–∞—Ä—ñ–∞–Ω—Ç {var_num} –∑ {copies_count}...")
                for arch_idx, arch in enumerate(archive_paths):
                    extract_dir = os.path.join(temp_rewritten, f"var_{var_num}_arch_{arch_idx}")
                    os.makedirs(extract_dir, exist_ok=True)
                    try:
                        with zipfile.ZipFile(arch, 'r') as z:
                            z.extractall(extract_dir)
                    except:
                        st.warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è —Ä–æ–∑–ø–∞–∫—É–≤–∞—Ç–∏ {os.path.basename(arch)}")
                        continue

                    html_files = [os.path.join(root, f) for root, _, fs in os.walk(extract_dir) for f in fs if f.lower().endswith('.html')]

                    lang = get_site_language(html_files)
                    st.info(f"–ú–æ–≤–∞ –¥–ª—è –≤–∞—Ä—ñ–∞–Ω—Ç–∞ {var_num} –∞—Ä—Ö—ñ–≤—É {os.path.basename(arch)}: {lang}")

                    new_site_name = unique_names[name_index]
                    name_index += 1

                    rewritten_count = 0
                    for html in html_files:
                        with open(html, 'r', encoding='utf-8', errors='ignore') as f:
                            content = f.read()
                        new_content = rewrite_content(client, content, lang, new_site_name)
                        with open(html, 'w', encoding='utf-8') as f:
                            f.write(new_content)
                        rewritten_count += 1

                    # –î–æ–¥–∞—î–º–æ –≤ –≥–æ–ª–æ–≤–Ω–∏–π –∞—Ä—Ö—ñ–≤
                    for root, _, files in os.walk(extract_dir):
                        for file in files:
                            full = os.path.join(root, file)
                            arc = os.path.relpath(full, temp_rewritten)
                            master_zip.write(full, arc)

                    st.info(f"–í–∞—Ä—ñ–∞–Ω—Ç {var_num} –∞—Ä—Ö—ñ–≤—É {arch_idx} –≥–æ—Ç–æ–≤–∏–π: {rewritten_count} —Å—Ç–æ—Ä—ñ–Ω–æ–∫ –ø–µ—Ä–µ–ø–∏—Å–∞–Ω–æ, –Ω–æ–≤–∞ –Ω–∞–∑–≤–∞ {new_site_name}")

                    progress.progress(0.1 + (var_num * (arch_idx+1)) / (copies_count * len(archive_paths)) * 0.9)

        st.session_state.result = {'success': True, 'master_archive_path': master_zip_path}
        st.session_state.processed = True
        st.rerun()

else:
    st.warning("–í–≤–µ–¥–∏ –∫–ª—é—á, —Ç–µ–º—É —ñ –∑–∞–≤–∞–Ω—Ç–∞–∂ –∞—Ä—Ö—ñ–≤–∏")

if st.session_state.processed and st.session_state.result:
    st.success("–ì–æ—Ç–æ–≤–æ! 5 —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤ –∑ —Ä–µ—Ä–∞–π—Ç–æ–º —ñ –Ω–∞–∑–≤–∞–º–∏.")
    with open(st.session_state.result['master_archive_path'], 'rb') as f:
        data = f.read()
    st.download_button(
        label="‚¨áÔ∏è –°–∫–∞—á–∞—Ç–∏ –≥–æ–ª–æ–≤–Ω–∏–π –∞—Ä—Ö—ñ–≤ (–≤—Å—ñ 5 –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤)",
        data=data,
        file_name="unique_rewritten_duplicates.zip",
        mime="application/zip"
    )
